version: '3.8'

services:
  inference-ui:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm_inference_ui
    ports:
      - "7861:7861"
    volumes:
      - huggingface_cache:/root/.cache/huggingface
      - ./inference_app.py:/app/inference_app.py
      - ./saves:/app/LLaMA-Factory/saves
      # --- NUEVA L√çNEA: Mapear el JSON ---
      - ./data.json:/app/data.json
    environment:
      - CUDA_VISIBLE_DEVICES=-1
      - OMP_NUM_THREADS=6
    command: sh -c "pip install gradio && python /app/inference_app.py"

volumes:
  huggingface_cache:
