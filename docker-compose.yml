version: '3.8'

services:
  inference-ui:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: llm_api_service
    ports:
      - "7861:7861"
    volumes:
      - huggingface_cache:/root/.cache/huggingface
      # Mapeamos el nuevo script
      - ./api_app.py:/app/api_app.py
      - ./saves:/app/LLaMA-Factory/saves
    environment:
      - CUDA_VISIBLE_DEVICES=-1
      - OMP_NUM_THREADS=6
    # Instalamos dependencias extras y corremos el script
    command: sh -c "pip install fastapi uvicorn gradio && python /app/api_app.py"

volumes:
  huggingface_cache:
