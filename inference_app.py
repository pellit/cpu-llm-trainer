
import gradio as gr
import torch
import json
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# --- CONFIGURACI√ìN ---
ADAPTER_PATH = "/app/LLaMA-Factory/saves/tu_modelo_entrenado"
BASE_MODEL_ID = "Qwen/Qwen2.5-0.5B-Instruct"
JSON_PATH = "/app/data.json"  # Ruta donde montaremos el JSON en Docker

print("‚è≥ Cargando modelo y datos...")

# 1. Cargar el JSON de contexto
try:
    with open(JSON_PATH, "r", encoding="utf-8") as f:
        json_data = json.load(f)
        json_str = json.dumps(json_data, indent=2, ensure_ascii=False)
        print("‚úÖ JSON cargado correctamente.")
except Exception as e:
    print(f"‚ö†Ô∏è Error cargando JSON: {e}")
    json_str = "{}"

# 2. Definir el System Prompt (La "Personalidad" y el Contexto)
SYSTEM_PROMPT = f"""
Eres un asistente de atenci√≥n al cliente √∫til y amable.
Tu objetivo es responder preguntas bas√°ndote ESTRICTAMENTE en la siguiente informaci√≥n en formato JSON.
Si la respuesta no est√° en el JSON, di amablemente que no tienes esa informaci√≥n.

INFORMACI√ìN DE CONTEXTO:
{json_str}
"""

# 3. Cargar Tokenizer y Modelo
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID)
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL_ID,
    dtype=torch.float32,
    device_map="cpu",
    low_cpu_mem_usage=True
)

try:
    model = PeftModel.from_pretrained(model, ADAPTER_PATH)
    print("‚úÖ Adaptadores LoRA cargados.")
except:
    print("‚ÑπÔ∏è Usando modelo base sin adaptadores.")

model.eval()

# --- L√ìGICA DEL CHAT ---
def generate_response(message, history):
    # 1. Construir la lista de mensajes
    # IMPORTANTE: El mensaje del sistema va PRIMERO
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]
    
    # 2. A√±adir historia del chat (Gradio ahora usa formato lista de dicts con type="messages")
    messages.extend(history)
    
    # 3. A√±adir el mensaje actual del usuario
    messages.append({"role": "user", "content": message})

    # 4. Aplicar plantilla de chat
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    inputs = tokenizer([text], return_tensors="pt").to("cpu")

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256, # Un poco m√°s de espacio para responder
            temperature=0.5,    # M√°s bajo para que sea m√°s fiel al JSON (menos alucinaci√≥n)
            do_sample=True,
            top_p=0.9
        )

    # Decodificar solo la respuesta nueva
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, outputs)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return response

# --- INTERFAZ GRADIO ---
demo = gr.ChatInterface(
    fn=generate_response,
    type="messages",  # Formato moderno de Gradio
    title="ü§ñ Asistente con Contexto JSON",
    description=f"Este asistente responde preguntas sobre: {json_str[:100]}...",
)

if __name__ == "__main__":
    demo.launch(server_name="0.0.0.0", server_port=7861)
